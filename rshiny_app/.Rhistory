}
color
for (i in 1:length(text_split)) {
if (!is.na(text_split[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
print(text_split[i:j])
if (sum(!is.na(text_split[i:j]))==2) {
condition = T
}
}
print('######')
if (condition) {
ngrams = ''
for (word in text_split[i:j]) {
ngrams = paste(ngrams, word)
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_text_lemmatizesplit[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
print(text_lemmatize[i:j])
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print('######')
if (condition) {
ngrams = ''
for (word in text_lemmatize[i:j]) {
ngrams = paste(ngrams, word)
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
print(text_lemmatize[i:j])
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print('######')
if (condition) {
ngrams = ''
for (word in text_lemmatize[i:j]) {
ngrams = paste(ngrams, word)
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
color
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
print(text_lemmatize[i:j])
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print('######')
if (condition) {
ngrams = ''
for (word in text_lemmatize[i:j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
print(text_lemmatize[i:j])
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print('######')
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[i+1:j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print(text_lemmatize[i:j])
print('######')
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[i+1:j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print(text_lemmatize[i:j])
print(i)
print(j)
print('######')
print(text_lemmatize[i+1:j])
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[i+1:j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print(text_lemmatize[i:j])
print(i)
print(j)
print('######')
print(text_lemmatize[(i+1):j])
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[i+1:j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
print(text_lemmatize[i:j])
print(i)
print(j)
print('######')
print(text_lemmatize[(i+1):j])
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[(i+1):j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
color = rep(0,length(text_split))
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[(i+1):j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
color
top_terms3 = c('demand healthcare services', 'government community emissions')
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 4) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==3) {
condition = T
}
}
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[(i+1):j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms3)) {
color[i:j]=0.5
}
}
}
}
color = rep(0,length(text_split))
top_terms3 = c('demand healthcare services', 'government community emissions')
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 4) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==3) {
condition = T
}
}
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[(i+1):j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms3)) {
color[i:j]=0.5
}
}
}
}
color
is_in("demand healthcare service", top_terms3)
top_terms3 = c('demand healthcare services', 'government community emissions')
top_terms3
"demand healthcare service" == top_terms3[1]
color = rep(0,length(text_split))
top_terms3 = c('demand healthcare service', 'government community emission')
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 4) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==3) {
condition = T
}
}
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[(i+1):j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms3)) {
color[i:j]=0.5
}
}
}
}
color
color = rep(0,length(text_split))
top_terms1 = c('healthcare', 'community')
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
print(text_lemmatize[i])
if (is_in(text_lemmatize[i],top_terms1)) {
color[i]=0.5
}
}
}
color
strsplit(as.character(a$body_preprocessed), " ")
length(strsplit(as.character(a$body_preprocessed), " "))
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
subset(df_lemma, token == 'solid')
subset(df_lemma, token == 'waste')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
text = 'increased demand healthcare services together government and community emissions'
text_split = str_split(text, ' ')[[1]]
text_lemmatize = copy(text_split)
for (i in 1:length(text_split)) {
text_lemmatize[i] = subset(df_lemma, token == text_split[i])$lem[1]
}
top_terms = c('demand healthcare', 'government community')
color = rep(0,length(text_split))
for (i in 1:length(text_lemmatize)) {
if (!is.na(text_lemmatize[i])) {
j = i
condition = F
while (condition == F & j < i + 3) {
j = j+1
if (sum(!is.na(text_lemmatize[i:j]))==2) {
condition = T
}
}
if (condition) {
ngrams = text_lemmatize[i]
for (word in text_lemmatize[(i+1):j]) {
if (!is.na(word)) {
ngrams = paste(ngrams, word)
}
}
print(ngrams)
if (is_in(ngrams,top_terms)) {
color[i:j]=0.5
}
}
}
}
color
text_lemmatize
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
length('fnzebifjzife')
nchar('vqrzegezrge')
nchar(c('ffzefzfzefz','fzefzfz'))
nchar(df_document_vector$terms_non_pre)
mean(nchar(df_document_vector$terms_non_pre))
df_document_vector[df_document_vector$terms_non_pre > mean(nchar(df_document_vector$terms_non_pre)),]
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
View(df_label)
df_document_vector_modele_cluster = subset(df_document_vector, modele == 'nmf_frobenius' & cluster == 22)
mean(nchar(df_document_vector_modele_cluster$terms_non_pre))
a = df_document_vector_modele_cluster[df_document_vector_modele_cluster$terms_non_pre > mean(nchar(df_document_vector_modele_cluster$terms_non_pre)),]
View(a)
df_document_vector_modele_cluster$terms_non_pre > mean(nchar(df_document_vector_modele_cluster$terms_non_pre))
nchar(df_document_vector_modele_cluster$terms_non_pre) > mean(nchar(df_document_vector_modele_cluster$terms_non_pre))
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
library(rsconnect)
deployApp("~/Documents/NLP-TopicModeling/Topic-visualization/rshiny_app")
shiny::runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
View(df_label)
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
View(df_hierarchy_top_terms)
sub_hier = subset(df_hierarchy_top_terms, modele == 'blend_model')
sub_lab = subset(df_label, modele == 'blend_modele')
b = merge(sub_hier, sub_lab, by = 'cluster')
View(datas)
View(b)
sub_lab = subset(df_label, modele == 'blend_modele' & choix == 1)
View(sub_hier)
View(sub_lab)
View(df_label)
sub_lab = subset(df_label, modele == 'blend_model' & choix == 1)
b = merge(sub_hier, sub_lab, by = 'cluster')
View(b)
b$label
get_top_terms = function(string_list, n){
" design the top terms presentation from a string_list of top terms "
string_list = gsub("'", "",string_list)
string_list = substring(string_list, 2)
string_list = strsplit(string_list,', ')
string_list = string_list[[1]]
message = ""
for (i in 1:n) {
if (i == 1) {
message = paste(message,string_list[i])
}
else{
message = paste(message, '-',string_list[i])
}
}
message = substring(message, 2)
return(message)
}
copy_df_hierarchy = copy(sub_hier)
copy_df_hierarchy$top_3 = unlist(lapply(copy_df_hierarchy$top_terms, get_top_terms, n=3))
View(copy_df_hierarchy)
View(sub_lab)
View(df_label)
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
View(copy_df_hierarchy)
View(df_hierarchy_top_terms)
runApp('Documents/NLP-TopicModeling/Topic-visualization/rshiny_app')
setwd("~/Documents/NLP-TopicModeling/Topic-visualization/rshiny_app")
library(rsconnect)
deployApp("~/Documents/NLP-TopicModeling/Topic-visualization/rshiny_app")
deployApp("~/Documents/NLP-TopicModeling/Topic-visualization/rshiny_app")
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
data
data
dt_topics$top_topic_terms_modele_cluster
dt_topics$top_topic_terms_modele_cluster$terms[1:4]
data$label
dt_topics$top_topic_terms_modele_cluster$terms[1:4][!dt_topics$top_topic_terms_modele_cluster$terms[1:4] %in% data$label]
dt_topics$top_topic_terms_modele_cluster$terms[1:4][!dt_topics$top_topic_terms_modele_cluster$terms[1:4] %in% data$label][1:1]
dt_topics$top_topic_terms_modele_cluster$terms[1:4][!dt_topics$top_topic_terms_modele_cluster$terms[1:4] %in% data$label][1:0]
choices = c(data$label,
dt_topics$top_topic_terms_modele_cluster$terms[1:4][!dt_topics$top_topic_terms_modele_cluster$terms[1:4] %in% data$label][1:(3-length(data$label))])
choices
choices = c(data$label,
dt_topics$top_topic_terms_modele_cluster$terms[1:4][!dt_topics$top_topic_terms_modele_cluster$terms[1:4] %in% data$label][1:(5-length(data$label))])
choices
shiny::runApp()
shiny::runApp()
shiny::runApp()
label_i
df_document_vector_reactive
df_word_cluster_i
input$select_nb_topterms
input$select_lim_top_termes
label_i
label_i
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
shiny::runApp()
